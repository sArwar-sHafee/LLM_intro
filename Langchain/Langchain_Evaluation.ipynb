{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OjRpVzmp0BH0",
        "LyC9EIsQDUfk",
        "gZFImbJdDYro",
        "PfCNtL7xDdGG",
        "ab4xBgNS0Ee7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "BCfwmrbr2gXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_community\n",
        "!pip install openai\n",
        "!pip install -U langchain-openai\n",
        "!pip install langchain_core"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6-xAkAFCzdBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Secrate Key\n",
        "import os #@markdown import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"YourKey\" #@markdown os.environ['OPENAI_API_KEY'] = \"YourKey\""
      ],
      "metadata": {
        "id": "Zmc4Dq17BPTg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criteria Evaluation"
      ],
      "metadata": {
        "id": "OjRpVzmp0BH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referenceless"
      ],
      "metadata": {
        "id": "LyC9EIsQDUfk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j__qRvCXzLAu"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "\n",
        "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\")\n",
        "\n",
        "# This is equivalent to loading using the enum\n",
        "from langchain.evaluation import EvaluatorType\n",
        "\n",
        "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = evaluator.evaluate_strings(\n",
        "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
        "    input=\"What's 2+2?\",\n",
        ")\n",
        "print(eval_result)"
      ],
      "metadata": {
        "id": "x44WWQCJzSCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f0f891-7ca0-4569-a892-27cf1bdf9aec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'reasoning': 'The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, the answer to the question \"What\\'s 2+2?\" is given as \"The answer you\\'re looking for is that two and two is four.\" However, before providing the answer, the respondent adds \"That\\'s an elementary question.\" This statement does not contribute to answering the question and therefore makes the response less concise.\\n\\nSo, the submission does not meet the criterion of conciseness.\\n\\nN', 'value': 'N', 'score': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Reference"
      ],
      "metadata": {
        "id": "gZFImbJdDYro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
        "\n",
        "# We can even override the model's learned knowledge using ground truth labels\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    input=\"What \",\n",
        "    prediction=\"New Dilhi\",\n",
        "    reference=\"Dhaka\",\n",
        ")\n",
        "print(f'With ground truth: {eval_result[\"score\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCRmx0QpBfy2",
        "outputId": "f9ad25f9-cf6b-466c-d3ca-87db3fc1282b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.chat_models.openai:WARNING! seed is not default parameter.\n",
            "                    seed was transferred to model_kwargs.\n",
            "                    Please confirm that seed is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With ground truth: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure LLM"
      ],
      "metadata": {
        "id": "PfCNtL7xDdGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "EthuuZfMB7E-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "JN19yxFJC8nN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = load_evaluator(\"labeled_criteria\", llm=llm, criteria=\"correctness\")"
      ],
      "metadata": {
        "id": "umBjNiw_DQBi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = evaluator.evaluate_strings(\n",
        "    input=\"What did I ate last night\",\n",
        "    prediction=\"Rice with catfish\",\n",
        "    reference=\"fish\",\n",
        ")\n",
        "print(f'With ground truth: {eval_result[\"score\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3bJ69OIDn5W",
        "outputId": "fc561d3e-c132-4fe9-c2eb-781c9b25dd18"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With ground truth: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With prompt"
      ],
      "metadata": {
        "id": "0aFmG6aOEPi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "fstring = \"\"\"Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response:\n",
        "\n",
        "Grading Rubric: {criteria}\n",
        "Expected Response: {reference}\n",
        "\n",
        "DATA:\n",
        "---------\n",
        "Question: {input}\n",
        "Response: {output}\n",
        "---------\n",
        "Write out your explanation for each criterion, then respond with Y or N on a new line.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(fstring)"
      ],
      "metadata": {
        "id": "Dfu7gZt6Dog8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "evaluator = load_evaluator(\"labeled_criteria\", llm=llm, criteria=\"correctness\", prompt=prompt)\n",
        "\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    input=\"What did I ate last night\",\n",
        "    prediction=\"Rice with catfish\",\n",
        "    reference=\"fish\",\n",
        ")\n",
        "print(f'With ground truth: {eval_result[\"score\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMgU4iAWER-O",
        "outputId": "96cb6d9a-c6e0-44be-d199-f95261a50c3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With ground truth: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Distance"
      ],
      "metadata": {
        "id": "ab4xBgNS0Ee7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "\n",
        "evaluator = load_evaluator(\"embedding_distance\")\n",
        "evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"She loves me\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQxh8wYc0Kb2",
        "outputId": "5c856021-003e-43e6-e15c-043ed7553398"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.22719140104367708}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I will go\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw_xGMPX0pxE",
        "outputId": "4c7d0e8e-54e0-435d-dcb5-8eca54e4c372"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.03772826064560364}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}