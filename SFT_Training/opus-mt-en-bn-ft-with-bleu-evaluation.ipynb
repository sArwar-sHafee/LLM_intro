{"cells":[{"cell_type":"markdown","metadata":{},"source":["# EnvSet"]},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-06-29T04:16:46.711181Z","iopub.status.busy":"2024-06-29T04:16:46.710456Z","iopub.status.idle":"2024-06-29T04:16:46.715287Z","shell.execute_reply":"2024-06-29T04:16:46.714289Z","shell.execute_reply.started":"2024-06-29T04:16:46.711147Z"},"trusted":true},"outputs":[],"source":["# !python -m venv env\n","# !source env/bin/activate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","!pip install tqdm transformers accelerate datasets sacrebleu evaluate sentencepiece sacremoses "]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:20:51.348459Z","iopub.status.busy":"2024-06-29T12:20:51.348129Z","iopub.status.idle":"2024-06-29T12:20:51.353216Z","shell.execute_reply":"2024-06-29T12:20:51.352359Z","shell.execute_reply.started":"2024-06-29T12:20:51.348431Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.19.2\n"]}],"source":["import datasets\n","print(datasets.__version__)"]},{"cell_type":"code","execution_count":4,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-06-29T12:20:51.355766Z","iopub.status.busy":"2024-06-29T12:20:51.355170Z","iopub.status.idle":"2024-06-29T12:20:54.975127Z","shell.execute_reply":"2024-06-29T12:20:54.974282Z","shell.execute_reply.started":"2024-06-29T12:20:51.355730Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset: 1000000\n","Validation dataset: 2000\n","Test dataset: 2000\n"]}],"source":["from datasets import load_dataset\n","\n","# Load the train, validation, and test splits explicitly\n","train_dataset = load_dataset(\"Helsinki-NLP/opus-100\", data_dir=\"bn-en\", split='train')\n","validation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", data_dir=\"bn-en\", split='validation')\n","test_dataset = load_dataset(\"Helsinki-NLP/opus-100\", data_dir=\"bn-en\", split='test')\n","\n","print(f\"Train dataset: {len(train_dataset)}\")\n","print(f\"Validation dataset: {len(validation_dataset)}\")\n","print(f\"Test dataset: {len(test_dataset)}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-06-29T12:20:54.976436Z","iopub.status.busy":"2024-06-29T12:20:54.976135Z","iopub.status.idle":"2024-06-29T12:21:11.843396Z","shell.execute_reply":"2024-06-29T12:21:11.842543Z","shell.execute_reply.started":"2024-06-29T12:20:54.976394Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-29 12:21:00.439452: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-29 12:21:00.439586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-29 12:21:00.569955: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import csv\n","import evaluate\n","from transformers import MarianConfig, MarianMTModel, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:21:25.350614Z","iopub.status.busy":"2024-06-29T12:21:25.349665Z","iopub.status.idle":"2024-06-29T12:21:31.350480Z","shell.execute_reply":"2024-06-29T12:21:31.349534Z","shell.execute_reply.started":"2024-06-29T12:21:25.350582Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd28e54325814a79818e85a6ec231771","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea807742d1874282ad227acee17d5df4","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90b554b2111347eda040f9453a144eba","version_major":2,"version_minor":0},"text/plain":["source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aca985f045a0449682cf5ecbe43d5da8","version_major":2,"version_minor":0},"text/plain":["target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d387627256e44a079c7357f4c471f4a3","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62d30dd205a5467d960d44bfeea24151","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2531d8a89c7c4a8ca6119748c3d2acd4","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-mul\")\n","#model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-mul\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"SarwarShafee/opus_mt_en-bn-ft\")\n","model = MarianMTModel.from_pretrained(\"SarwarShafee/opus_mt_en-bn-ft\")\n","\n","# set special tokens, not sure if it's needed but adding them for sanity...\n","model.config.eos_token_id = tokenizer.eos_token_id\n","model.config.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:21:45.750857Z","iopub.status.busy":"2024-06-29T12:21:45.750088Z","iopub.status.idle":"2024-06-29T12:21:45.758030Z","shell.execute_reply":"2024-06-29T12:21:45.756894Z","shell.execute_reply.started":"2024-06-29T12:21:45.750824Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['translation'],\n","    num_rows: 1000000\n","})\n","{'bn': 'à¦¹à§à¦¯à¦¾à¦?', 'en': 'Yeah?'}\n"]}],"source":["print(train_dataset)\n","print(train_dataset[0]['translation'])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:25:04.620825Z","iopub.status.busy":"2024-06-29T12:25:04.619913Z","iopub.status.idle":"2024-06-29T12:25:04.625771Z","shell.execute_reply":"2024-06-29T12:25:04.624770Z","shell.execute_reply.started":"2024-06-29T12:25:04.620789Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['en', 'bn'],\n","    num_rows: 1000000\n","}) Dataset({\n","    features: ['en', 'bn'],\n","    num_rows: 2000\n","})\n"]}],"source":["def split_translation(example):\n","    return {\n","        'en': example['translation']['en'],\n","        'bn': example['translation']['bn']\n","    }\n","\n","train_mapped_dataset = train_dataset.map(split_translation)\n","\n","tr_mapd_dt = train_mapped_dataset.remove_columns(['translation'])\n","\n","val_mapped_dataset = validation_dataset.map(split_translation)\n","\n","val_mapd_dt = val_mapped_dataset.remove_columns(['translation'])\n","\n","print(tr_mapd_dt, val_mapd_dt)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Then, we convert the string inputs to vocabulary IDs"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:25:15.699059Z","iopub.status.busy":"2024-06-29T12:25:15.698157Z","iopub.status.idle":"2024-06-29T12:25:15.706058Z","shell.execute_reply":"2024-06-29T12:25:15.704835Z","shell.execute_reply.started":"2024-06-29T12:25:15.699026Z"},"trusted":true},"outputs":[{"data":{"text/plain":["MarianTokenizer(name_or_path='SarwarShafee/opus_mt_en-bn-ft', vocab_size=64110, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t64109: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:25:23.070771Z","iopub.status.busy":"2024-06-29T12:25:23.070379Z","iopub.status.idle":"2024-06-29T12:31:23.328551Z","shell.execute_reply":"2024-06-29T12:31:23.327651Z","shell.execute_reply.started":"2024-06-29T12:25:23.070741Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78bdeb3c698e487b940309d53f9de06b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d442c1885424ccf8f45242d4cb6fcca","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def preprocess_function(batch):\n","    inputs = tokenizer(batch['en'], max_length=64, truncation=True, padding=\"max_length\")\n","    outputs = tokenizer(batch['bn'], max_length=64, truncation=True, padding=\"max_length\")\n","\n","    return {\"input_ids\": inputs[\"input_ids\"], \n","            \"labels\": outputs.input_ids.copy()}\n","\n","train_data_with_token = tr_mapd_dt.map(preprocess_function, batched=True, batch_size=1000)\n","val_data_with_token  = val_mapd_dt.map(preprocess_function, batched=True, batch_size=1000)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:31:23.331259Z","iopub.status.busy":"2024-06-29T12:31:23.330496Z","iopub.status.idle":"2024-06-29T12:31:23.337298Z","shell.execute_reply":"2024-06-29T12:31:23.336289Z","shell.execute_reply.started":"2024-06-29T12:31:23.331224Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['en', 'bn', 'input_ids', 'labels'],\n","    num_rows: 1000000\n","})\n","{'en': 'Illegal combatant on the Grid.', 'bn': 'à¦—à§à¦°à¦¿à¦¡à§‡ à¦…à¦¬à§ˆà¦§ à¦¯à§à¦¦à§à¦§à¦¾à¥¤', 'input_ids': [49018, 22511, 2208, 40, 5, 31851, 2, 0, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109], 'labels': [4, 1, 1185, 1380, 1, 4, 1, 4, 15514, 1267, 4376, 1, 2216, 1717, 0, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109]}\n"]}],"source":["print(train_data_with_token)\n","print(train_data_with_token[100000])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:31:23.338702Z","iopub.status.busy":"2024-06-29T12:31:23.338435Z","iopub.status.idle":"2024-06-29T12:31:23.443763Z","shell.execute_reply":"2024-06-29T12:31:23.442775Z","shell.execute_reply.started":"2024-06-29T12:31:23.338680Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<pad>\n","<unk>\n"]}],"source":["print(tokenizer.convert_ids_to_tokens(64109))\n","print(tokenizer.convert_ids_to_tokens(64171))"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluator"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:31:23.447812Z","iopub.status.busy":"2024-06-29T12:31:23.447204Z","iopub.status.idle":"2024-06-29T12:31:24.861551Z","shell.execute_reply":"2024-06-29T12:31:24.860816Z","shell.execute_reply.started":"2024-06-29T12:31:23.447787Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1aaf31ca8a994f60adf653a7f32dc1f4","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df539f71251e46a98ad6a39f5ce55dfb","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd557ba74e194acf85f83a973fd31f43","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aef620e869aa4be9a3f8782ced700d8c","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/9.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["mt_metrics = evaluate.combine(\n","    [\"bleu\", \"chrf\"], force_prefix=True\n",")\n","\n","def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","    \n","    predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n","    references = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    outputs = mt_metrics.compute(predictions=predictions,\n","                             references=references)\n","\n","    return outputs"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:31:24.863126Z","iopub.status.busy":"2024-06-29T12:31:24.862541Z","iopub.status.idle":"2024-06-29T12:31:25.988001Z","shell.execute_reply":"2024-06-29T12:31:25.987108Z","shell.execute_reply.started":"2024-06-29T12:31:24.863101Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'score': 50.043063606582294, 'char_order': 6, 'word_order': 0, 'beta': 2}\n","------------------------------------------------\n","{'bleu_bleu': 0.45067506321061157, 'bleu_precisions': [0.7058823529411765, 0.42857142857142855, 0.36363636363636365, 0.375], 'bleu_brevity_penalty': 1.0, 'bleu_length_ratio': 1.0, 'bleu_translation_length': 17, 'bleu_reference_length': 17, 'chr_f_score': 50.043063606582294, 'chr_f_char_order': 6, 'chr_f_word_order': 0, 'chr_f_beta': 2}\n"]}],"source":["hyp = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n","ref = ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.']\n","\n","#chrf = evaluate.load('chrf')\n","#print(chrf.compute(predictions=hyp, references=ref))\n","\n","chrf = evaluate.load('chrf', force_prefix=True)\n","print(chrf.compute(predictions=hyp, references=ref))\n","\n","print(\"------------------------------------------------\")\n","\n","mt_metrics = evaluate.combine(\n","    [\"bleu\", \"chrf\"], force_prefix=True\n",")\n","print(mt_metrics.compute(predictions=hyp, references=ref))"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:31:25.989547Z","iopub.status.busy":"2024-06-29T12:31:25.989213Z","iopub.status.idle":"2024-06-29T12:33:16.105431Z","shell.execute_reply":"2024-06-29T12:33:16.104512Z","shell.execute_reply.started":"2024-06-29T12:31:25.989519Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240629_123259-jc4qg5hx</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx' target=\"_blank\">breezy-monkey-3</a></strong> to <a href='https://wandb.ai/ss-organization/opus-mt-en-bn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/ss-organization/opus-mt-en-bn' target=\"_blank\">https://wandb.ai/ss-organization/opus-mt-en-bn</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx' target=\"_blank\">https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a57804699c0>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.init(project=\"opus-mt-en-bn\")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:44:02.177736Z","iopub.status.busy":"2024-06-29T12:44:02.177349Z","iopub.status.idle":"2024-06-29T12:44:02.217923Z","shell.execute_reply":"2024-06-29T12:44:02.216243Z","shell.execute_reply.started":"2024-06-29T12:44:02.177712Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir='opus-mt-en-bn',\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    logging_steps=1,\n","    save_steps=10,\n","    eval_steps=10,\n","    max_steps=800,\n","    evaluation_strategy=\"steps\",\n","    predict_with_generate=True,\n","    report_to=[\"wandb\"],\n","    metric_for_best_model=\"chr_f_score\",\n","    load_best_model_at_end=True,\n","    save_total_limit=3,\n","    learning_rate=5e-5 # If I don't mention it, it will be 5e-5 by default\n",")\n","\n","# To use multiple gpu: \n","\"\"\"n_gpu=-1  # Use all available GPUs\"\"\"\n","\n","# To upload to huggingface:\n","\"\"\"push_to_hub=True   # Repo will be same as output directory\"\"\""]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:44:34.166941Z","iopub.status.busy":"2024-06-29T12:44:34.166538Z","iopub.status.idle":"2024-06-29T12:44:34.190193Z","shell.execute_reply":"2024-06-29T12:44:34.189353Z","shell.execute_reply.started":"2024-06-29T12:44:34.166913Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_data_with_token.with_format(\"torch\"),\n","    eval_dataset=val_data_with_token.with_format(\"torch\"),\n","    compute_metrics=compute_metrics,\n",")\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T12:44:42.579585Z","iopub.status.busy":"2024-06-29T12:44:42.578941Z","iopub.status.idle":"2024-06-29T15:06:34.607982Z","shell.execute_reply":"2024-06-29T15:06:34.605838Z","shell.execute_reply.started":"2024-06-29T12:44:42.579552Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='231' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [231/800 2:18:35 < 5:44:21, 0.03 it/s, Epoch 0.01/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu Bleu</th>\n","      <th>Bleu Precisions</th>\n","      <th>Bleu Brevity Penalty</th>\n","      <th>Bleu Length Ratio</th>\n","      <th>Bleu Translation Length</th>\n","      <th>Bleu Reference Length</th>\n","      <th>Chr F Score</th>\n","      <th>Chr F Char Order</th>\n","      <th>Chr F Word Order</th>\n","      <th>Chr F Beta</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.951400</td>\n","      <td>1.303110</td>\n","      <td>0.016623</td>\n","      <td>[0.14814814814814814, 0.02433834286474265, 0.008798102670032898, 0.0024068461401319306]</td>\n","      <td>1.000000</td>\n","      <td>1.022077</td>\n","      <td>17037</td>\n","      <td>16669</td>\n","      <td>15.701121</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.201400</td>\n","      <td>1.245765</td>\n","      <td>0.020289</td>\n","      <td>[0.18631150015092063, 0.03648277694146817, 0.014468264864294013, 0.004832761032684726]</td>\n","      <td>0.772713</td>\n","      <td>0.795009</td>\n","      <td>13252</td>\n","      <td>16669</td>\n","      <td>13.680195</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.971700</td>\n","      <td>1.216255</td>\n","      <td>0.021704</td>\n","      <td>[0.20521357804125387, 0.04192021636240703, 0.017399482718081356, 0.006100217864923747]</td>\n","      <td>0.702112</td>\n","      <td>0.738737</td>\n","      <td>12314</td>\n","      <td>16669</td>\n","      <td>14.005456</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.185000</td>\n","      <td>1.188353</td>\n","      <td>0.020099</td>\n","      <td>[0.20920464700625557, 0.04353917448000862, 0.018314532183145323, 0.006935270805812417]</td>\n","      <td>0.612851</td>\n","      <td>0.671306</td>\n","      <td>11190</td>\n","      <td>16669</td>\n","      <td>12.681768</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.944400</td>\n","      <td>1.161568</td>\n","      <td>0.022702</td>\n","      <td>[0.196672261548566, 0.03937276291119823, 0.015348648099207156, 0.005311920938851143]</td>\n","      <td>0.805374</td>\n","      <td>0.822065</td>\n","      <td>13703</td>\n","      <td>16669</td>\n","      <td>15.633141</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.271200</td>\n","      <td>1.139614</td>\n","      <td>0.023969</td>\n","      <td>[0.20201430448109764, 0.042071197411003236, 0.01632528898803488, 0.005655969506947006]</td>\n","      <td>0.805302</td>\n","      <td>0.822005</td>\n","      <td>13702</td>\n","      <td>16669</td>\n","      <td>15.948955</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.067900</td>\n","      <td>1.118648</td>\n","      <td>0.025677</td>\n","      <td>[0.2055050960271788, 0.04298660678482637, 0.016241737488196413, 0.005648441030275644]</td>\n","      <td>0.855796</td>\n","      <td>0.865259</td>\n","      <td>14423</td>\n","      <td>16669</td>\n","      <td>15.489040</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.234700</td>\n","      <td>1.112166</td>\n","      <td>0.024984</td>\n","      <td>[0.2130041518386714, 0.04444636977993415, 0.017314232711532516, 0.006105158235733865]</td>\n","      <td>0.789908</td>\n","      <td>0.809167</td>\n","      <td>13488</td>\n","      <td>16669</td>\n","      <td>15.680401</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.969800</td>\n","      <td>1.102233</td>\n","      <td>0.025670</td>\n","      <td>[0.18991006529505974, 0.038207613428852365, 0.01442660363517809, 0.004617604617604618]</td>\n","      <td>0.973560</td>\n","      <td>0.973904</td>\n","      <td>16234</td>\n","      <td>16669</td>\n","      <td>18.810749</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.075900</td>\n","      <td>1.086269</td>\n","      <td>0.026364</td>\n","      <td>[0.20686092167803266, 0.045200892857142856, 0.016842303349642453, 0.005550521069324876]</td>\n","      <td>0.862224</td>\n","      <td>0.870898</td>\n","      <td>14517</td>\n","      <td>16669</td>\n","      <td>17.229205</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>1.072200</td>\n","      <td>1.072942</td>\n","      <td>0.026595</td>\n","      <td>[0.19681834998150202, 0.040823496346261944, 0.01481843347988927, 0.004696185547249377]</td>\n","      <td>0.972574</td>\n","      <td>0.972944</td>\n","      <td>16218</td>\n","      <td>16669</td>\n","      <td>18.443563</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.168900</td>\n","      <td>1.061040</td>\n","      <td>0.026375</td>\n","      <td>[0.1901267903263677, 0.0386839481555334, 0.013911182450508293, 0.004730031236055332]</td>\n","      <td>1.000000</td>\n","      <td>1.022017</td>\n","      <td>17036</td>\n","      <td>16669</td>\n","      <td>18.522093</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>1.120000</td>\n","      <td>1.053778</td>\n","      <td>0.031750</td>\n","      <td>[0.19557725445146468, 0.04431036719865058, 0.017624749014650106, 0.006652842578192501]</td>\n","      <td>1.000000</td>\n","      <td>1.044454</td>\n","      <td>17410</td>\n","      <td>16669</td>\n","      <td>19.757075</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.854600</td>\n","      <td>1.043064</td>\n","      <td>0.030420</td>\n","      <td>[0.2085095794973874, 0.045735388111639796, 0.01774952530339305, 0.005863383172090296]</td>\n","      <td>0.963785</td>\n","      <td>0.964425</td>\n","      <td>16076</td>\n","      <td>16669</td>\n","      <td>19.192813</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.133600</td>\n","      <td>1.034731</td>\n","      <td>0.030361</td>\n","      <td>[0.20001138368717628, 0.043732340097611094, 0.016702229416525643, 0.005816439996578565]</td>\n","      <td>1.000000</td>\n","      <td>1.053992</td>\n","      <td>17569</td>\n","      <td>16669</td>\n","      <td>19.790404</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.926100</td>\n","      <td>1.025766</td>\n","      <td>0.035405</td>\n","      <td>[0.21200345423143352, 0.05008088907645776, 0.02048645119164218, 0.008086253369272238]</td>\n","      <td>0.972205</td>\n","      <td>0.972584</td>\n","      <td>16212</td>\n","      <td>16669</td>\n","      <td>19.632532</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>1.149500</td>\n","      <td>1.024901</td>\n","      <td>0.033908</td>\n","      <td>[0.22577696526508226, 0.052082551594746715, 0.02032198469253101, 0.007876496534341524]</td>\n","      <td>0.915450</td>\n","      <td>0.918831</td>\n","      <td>15316</td>\n","      <td>16669</td>\n","      <td>19.808498</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>1.102300</td>\n","      <td>1.016426</td>\n","      <td>0.034259</td>\n","      <td>[0.20798043659787666, 0.048121827411167516, 0.019280305987042386, 0.007138934651290499]</td>\n","      <td>1.000000</td>\n","      <td>1.005819</td>\n","      <td>16766</td>\n","      <td>16669</td>\n","      <td>20.359816</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>1.087900</td>\n","      <td>1.007875</td>\n","      <td>0.034136</td>\n","      <td>[0.2160260201064459, 0.05135425046929472, 0.019315460094259444, 0.006336561962523762]</td>\n","      <td>1.000000</td>\n","      <td>1.014458</td>\n","      <td>16910</td>\n","      <td>16669</td>\n","      <td>21.040576</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.887000</td>\n","      <td>0.996157</td>\n","      <td>0.034297</td>\n","      <td>[0.22404580152671755, 0.053609148517736176, 0.020732432662078342, 0.007074279939363315]</td>\n","      <td>0.941417</td>\n","      <td>0.943068</td>\n","      <td>15720</td>\n","      <td>16669</td>\n","      <td>20.318746</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.981000</td>\n","      <td>0.994868</td>\n","      <td>0.038006</td>\n","      <td>[0.22301548886737657, 0.05449291316912068, 0.021559268098647572, 0.008239700374531835]</td>\n","      <td>0.991505</td>\n","      <td>0.991541</td>\n","      <td>16528</td>\n","      <td>16669</td>\n","      <td>20.993375</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.978800</td>\n","      <td>0.986320</td>\n","      <td>0.035880</td>\n","      <td>[0.2172908560311284, 0.050096872405203434, 0.02026918763018747, 0.007926023778071334]</td>\n","      <td>0.986654</td>\n","      <td>0.986742</td>\n","      <td>16448</td>\n","      <td>16669</td>\n","      <td>20.268620</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.908700</td>\n","      <td>0.985702</td>\n","      <td>0.036798</td>\n","      <td>[0.2252099786887301, 0.05481120584652863, 0.021934945788156798, 0.008101165777514326]</td>\n","      <td>0.956173</td>\n","      <td>0.957106</td>\n","      <td>15954</td>\n","      <td>16669</td>\n","      <td>20.763208</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"ename":"RuntimeError","evalue":"CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3241\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m-> 3241\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:159\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Load"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T11:22:58.438733Z","iopub.status.busy":"2024-06-29T11:22:58.438074Z","iopub.status.idle":"2024-06-29T11:22:58.445297Z","shell.execute_reply":"2024-06-29T11:22:58.444136Z","shell.execute_reply.started":"2024-06-29T11:22:58.438694Z"},"trusted":true},"outputs":[],"source":["from transformers import pipeline\n","\n","translate = pipeline('translation', model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T11:21:11.946571Z","iopub.status.busy":"2024-06-29T11:21:11.946077Z","iopub.status.idle":"2024-06-29T11:21:12.205073Z","shell.execute_reply":"2024-06-29T11:21:12.203866Z","shell.execute_reply.started":"2024-06-29T11:21:11.946533Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[{'translation_text': 'à¦†à¦®à¦¿ à¦¦à¦¿à¦¤à§‡ à¦®à¦¤à§‡ à¦®à§à¦°à§à¦£'}]"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["translate(\"I will die soon\")"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T11:56:48.510634Z","iopub.status.busy":"2024-06-29T11:56:48.510166Z","iopub.status.idle":"2024-06-29T11:56:48.518839Z","shell.execute_reply":"2024-06-29T11:56:48.517695Z","shell.execute_reply.started":"2024-06-29T11:56:48.510596Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-mul', vocab_size=64110, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t64109: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n"]}],"source":["print(tokenizer)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
