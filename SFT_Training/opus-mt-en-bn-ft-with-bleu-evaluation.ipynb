{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EnvSet","metadata":{}},{"cell_type":"code","source":"# !python -m venv env\n# !source env/bin/activate","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:16:46.710456Z","iopub.execute_input":"2024-06-29T04:16:46.711181Z","iopub.status.idle":"2024-06-29T04:16:46.715287Z","shell.execute_reply.started":"2024-06-29T04:16:46.711147Z","shell.execute_reply":"2024-06-29T04:16:46.714289Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install tqdm transformers accelerate datasets sacrebleu evaluate sentencepiece sacremoses ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"import datasets\nprint(datasets.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:20:51.348129Z","iopub.execute_input":"2024-06-29T12:20:51.348459Z","iopub.status.idle":"2024-06-29T12:20:51.353216Z","shell.execute_reply.started":"2024-06-29T12:20:51.348431Z","shell.execute_reply":"2024-06-29T12:20:51.352359Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2.19.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the train, validation, and test splits explicitly\ntrain_dataset = load_dataset(\"Helsinki-NLP/opus-100\", data_dir=\"bn-en\", split='train')\nvalidation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", data_dir=\"bn-en\", split='validation')\ntest_dataset = load_dataset(\"Helsinki-NLP/opus-100\", data_dir=\"bn-en\", split='test')\n\nprint(f\"Train dataset: {len(train_dataset)}\")\nprint(f\"Validation dataset: {len(validation_dataset)}\")\nprint(f\"Test dataset: {len(test_dataset)}\")\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-29T12:20:51.355170Z","iopub.execute_input":"2024-06-29T12:20:51.355766Z","iopub.status.idle":"2024-06-29T12:20:54.975127Z","shell.execute_reply.started":"2024-06-29T12:20:51.355730Z","shell.execute_reply":"2024-06-29T12:20:54.974282Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Train dataset: 1000000\nValidation dataset: 2000\nTest dataset: 2000\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nimport evaluate\nfrom transformers import MarianConfig, MarianMTModel, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-29T12:20:54.976135Z","iopub.execute_input":"2024-06-29T12:20:54.976436Z","iopub.status.idle":"2024-06-29T12:21:11.843396Z","shell.execute_reply.started":"2024-06-29T12:20:54.976394Z","shell.execute_reply":"2024-06-29T12:21:11.842543Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-06-29 12:21:00.439452: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-29 12:21:00.439586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-29 12:21:00.569955: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"#tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-mul\")\n#model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-mul\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"SarwarShafee/opus_mt_en-bn-ft\")\nmodel = MarianMTModel.from_pretrained(\"SarwarShafee/opus_mt_en-bn-ft\")\n\n# set special tokens, not sure if it's needed but adding them for sanity...\nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:21:25.349665Z","iopub.execute_input":"2024-06-29T12:21:25.350614Z","iopub.status.idle":"2024-06-29T12:21:31.350480Z","shell.execute_reply.started":"2024-06-29T12:21:25.350582Z","shell.execute_reply":"2024-06-29T12:21:31.349534Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd28e54325814a79818e85a6ec231771"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea807742d1874282ad227acee17d5df4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b554b2111347eda040f9453a144eba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aca985f045a0449682cf5ecbe43d5da8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d387627256e44a079c7357f4c471f4a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d30dd205a5467d960d44bfeea24151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2531d8a89c7c4a8ca6119748c3d2acd4"}},"metadata":{}}]},{"cell_type":"code","source":"print(train_dataset)\nprint(train_dataset[0]['translation'])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:21:45.750088Z","iopub.execute_input":"2024-06-29T12:21:45.750857Z","iopub.status.idle":"2024-06-29T12:21:45.758030Z","shell.execute_reply.started":"2024-06-29T12:21:45.750824Z","shell.execute_reply":"2024-06-29T12:21:45.756894Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['translation'],\n    num_rows: 1000000\n})\n{'bn': 'হ্যাঁ?', 'en': 'Yeah?'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def split_translation(example):\n    return {\n        'en': example['translation']['en'],\n        'bn': example['translation']['bn']\n    }\n\ntrain_mapped_dataset = train_dataset.map(split_translation)\n\ntr_mapd_dt = train_mapped_dataset.remove_columns(['translation'])\n\nval_mapped_dataset = validation_dataset.map(split_translation)\n\nval_mapd_dt = val_mapped_dataset.remove_columns(['translation'])\n\nprint(tr_mapd_dt, val_mapd_dt)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:25:04.619913Z","iopub.execute_input":"2024-06-29T12:25:04.620825Z","iopub.status.idle":"2024-06-29T12:25:04.625771Z","shell.execute_reply.started":"2024-06-29T12:25:04.620789Z","shell.execute_reply":"2024-06-29T12:25:04.624770Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['en', 'bn'],\n    num_rows: 1000000\n}) Dataset({\n    features: ['en', 'bn'],\n    num_rows: 2000\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Then, we convert the string inputs to vocabulary IDs","metadata":{}},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:25:15.698157Z","iopub.execute_input":"2024-06-29T12:25:15.699059Z","iopub.status.idle":"2024-06-29T12:25:15.706058Z","shell.execute_reply.started":"2024-06-29T12:25:15.699026Z","shell.execute_reply":"2024-06-29T12:25:15.704835Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"MarianTokenizer(name_or_path='SarwarShafee/opus_mt_en-bn-ft', vocab_size=64110, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t64109: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(batch):\n    inputs = tokenizer(batch['en'], max_length=64, truncation=True, padding=\"max_length\")\n    outputs = tokenizer(batch['bn'], max_length=64, truncation=True, padding=\"max_length\")\n\n    return {\"input_ids\": inputs[\"input_ids\"], \n            \"labels\": outputs.input_ids.copy()}\n\ntrain_data_with_token = tr_mapd_dt.map(preprocess_function, batched=True, batch_size=1000)\nval_data_with_token  = val_mapd_dt.map(preprocess_function, batched=True, batch_size=1000)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:25:23.070379Z","iopub.execute_input":"2024-06-29T12:25:23.070771Z","iopub.status.idle":"2024-06-29T12:31:23.328551Z","shell.execute_reply.started":"2024-06-29T12:25:23.070741Z","shell.execute_reply":"2024-06-29T12:31:23.327651Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78bdeb3c698e487b940309d53f9de06b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d442c1885424ccf8f45242d4cb6fcca"}},"metadata":{}}]},{"cell_type":"code","source":"print(train_data_with_token)\nprint(train_data_with_token[100000])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:31:23.330496Z","iopub.execute_input":"2024-06-29T12:31:23.331259Z","iopub.status.idle":"2024-06-29T12:31:23.337298Z","shell.execute_reply.started":"2024-06-29T12:31:23.331224Z","shell.execute_reply":"2024-06-29T12:31:23.336289Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['en', 'bn', 'input_ids', 'labels'],\n    num_rows: 1000000\n})\n{'en': 'Illegal combatant on the Grid.', 'bn': 'গ্রিডে অবৈধ যুদ্ধা।', 'input_ids': [49018, 22511, 2208, 40, 5, 31851, 2, 0, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109], 'labels': [4, 1, 1185, 1380, 1, 4, 1, 4, 15514, 1267, 4376, 1, 2216, 1717, 0, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109, 64109]}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens(64109))\nprint(tokenizer.convert_ids_to_tokens(64171))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:31:23.338435Z","iopub.execute_input":"2024-06-29T12:31:23.338702Z","iopub.status.idle":"2024-06-29T12:31:23.443763Z","shell.execute_reply.started":"2024-06-29T12:31:23.338680Z","shell.execute_reply":"2024-06-29T12:31:23.442775Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<pad>\n<unk>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluator","metadata":{}},{"cell_type":"code","source":"mt_metrics = evaluate.combine(\n    [\"bleu\", \"chrf\"], force_prefix=True\n)\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n    \n    predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    references = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    outputs = mt_metrics.compute(predictions=predictions,\n                             references=references)\n\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:31:23.447204Z","iopub.execute_input":"2024-06-29T12:31:23.447812Z","iopub.status.idle":"2024-06-29T12:31:24.861551Z","shell.execute_reply.started":"2024-06-29T12:31:23.447787Z","shell.execute_reply":"2024-06-29T12:31:24.860816Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aaf31ca8a994f60adf653a7f32dc1f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df539f71251e46a98ad6a39f5ce55dfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd557ba74e194acf85f83a973fd31f43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef620e869aa4be9a3f8782ced700d8c"}},"metadata":{}}]},{"cell_type":"code","source":"hyp = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\nref = ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.']\n\n#chrf = evaluate.load('chrf')\n#print(chrf.compute(predictions=hyp, references=ref))\n\nchrf = evaluate.load('chrf', force_prefix=True)\nprint(chrf.compute(predictions=hyp, references=ref))\n\nprint(\"------------------------------------------------\")\n\nmt_metrics = evaluate.combine(\n    [\"bleu\", \"chrf\"], force_prefix=True\n)\nprint(mt_metrics.compute(predictions=hyp, references=ref))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:31:24.862541Z","iopub.execute_input":"2024-06-29T12:31:24.863126Z","iopub.status.idle":"2024-06-29T12:31:25.988001Z","shell.execute_reply.started":"2024-06-29T12:31:24.863101Z","shell.execute_reply":"2024-06-29T12:31:25.987108Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"{'score': 50.043063606582294, 'char_order': 6, 'word_order': 0, 'beta': 2}\n------------------------------------------------\n{'bleu_bleu': 0.45067506321061157, 'bleu_precisions': [0.7058823529411765, 0.42857142857142855, 0.36363636363636365, 0.375], 'bleu_brevity_penalty': 1.0, 'bleu_length_ratio': 1.0, 'bleu_translation_length': 17, 'bleu_reference_length': 17, 'chr_f_score': 50.043063606582294, 'chr_f_char_order': 6, 'chr_f_word_order': 0, 'chr_f_beta': 2}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"opus-mt-en-bn\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:31:25.989213Z","iopub.execute_input":"2024-06-29T12:31:25.989547Z","iopub.status.idle":"2024-06-29T12:33:16.105431Z","shell.execute_reply.started":"2024-06-29T12:31:25.989519Z","shell.execute_reply":"2024-06-29T12:33:16.104512Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240629_123259-jc4qg5hx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx' target=\"_blank\">breezy-monkey-3</a></strong> to <a href='https://wandb.ai/ss-organization/opus-mt-en-bn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ss-organization/opus-mt-en-bn' target=\"_blank\">https://wandb.ai/ss-organization/opus-mt-en-bn</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx' target=\"_blank\">https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx</a>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ss-organization/opus-mt-en-bn/runs/jc4qg5hx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7a57804699c0>"},"metadata":{}}]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir='opus-mt-en-bn',\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_steps=1,\n    save_steps=10,\n    eval_steps=10,\n    max_steps=800,\n    evaluation_strategy=\"steps\",\n    predict_with_generate=True,\n    report_to=[\"wandb\"],\n    metric_for_best_model=\"chr_f_score\",\n    load_best_model_at_end=True,\n    save_total_limit=3,\n    learning_rate=5e-5 # If I don't mention it, it will be 5e-5 by default\n)\n\n# To use multiple gpu: \n\"\"\"n_gpu=-1  # Use all available GPUs\"\"\"\n\n# To upload to huggingface:\n\"\"\"push_to_hub=True   # Repo will be same as output directory\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:44:02.177349Z","iopub.execute_input":"2024-06-29T12:44:02.177736Z","iopub.status.idle":"2024-06-29T12:44:02.217923Z","shell.execute_reply.started":"2024-06-29T12:44:02.177712Z","shell.execute_reply":"2024-06-29T12:44:02.216243Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data_with_token.with_format(\"torch\"),\n    eval_dataset=val_data_with_token.with_format(\"torch\"),\n    compute_metrics=compute_metrics,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:44:34.166538Z","iopub.execute_input":"2024-06-29T12:44:34.166941Z","iopub.status.idle":"2024-06-29T12:44:34.190193Z","shell.execute_reply.started":"2024-06-29T12:44:34.166913Z","shell.execute_reply":"2024-06-29T12:44:34.189353Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:44:42.578941Z","iopub.execute_input":"2024-06-29T12:44:42.579585Z","iopub.status.idle":"2024-06-29T15:06:34.607982Z","shell.execute_reply.started":"2024-06-29T12:44:42.579552Z","shell.execute_reply":"2024-06-29T15:06:34.605838Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='231' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [231/800 2:18:35 < 5:44:21, 0.03 it/s, Epoch 0.01/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu Bleu</th>\n      <th>Bleu Precisions</th>\n      <th>Bleu Brevity Penalty</th>\n      <th>Bleu Length Ratio</th>\n      <th>Bleu Translation Length</th>\n      <th>Bleu Reference Length</th>\n      <th>Chr F Score</th>\n      <th>Chr F Char Order</th>\n      <th>Chr F Word Order</th>\n      <th>Chr F Beta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.951400</td>\n      <td>1.303110</td>\n      <td>0.016623</td>\n      <td>[0.14814814814814814, 0.02433834286474265, 0.008798102670032898, 0.0024068461401319306]</td>\n      <td>1.000000</td>\n      <td>1.022077</td>\n      <td>17037</td>\n      <td>16669</td>\n      <td>15.701121</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.201400</td>\n      <td>1.245765</td>\n      <td>0.020289</td>\n      <td>[0.18631150015092063, 0.03648277694146817, 0.014468264864294013, 0.004832761032684726]</td>\n      <td>0.772713</td>\n      <td>0.795009</td>\n      <td>13252</td>\n      <td>16669</td>\n      <td>13.680195</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.971700</td>\n      <td>1.216255</td>\n      <td>0.021704</td>\n      <td>[0.20521357804125387, 0.04192021636240703, 0.017399482718081356, 0.006100217864923747]</td>\n      <td>0.702112</td>\n      <td>0.738737</td>\n      <td>12314</td>\n      <td>16669</td>\n      <td>14.005456</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.185000</td>\n      <td>1.188353</td>\n      <td>0.020099</td>\n      <td>[0.20920464700625557, 0.04353917448000862, 0.018314532183145323, 0.006935270805812417]</td>\n      <td>0.612851</td>\n      <td>0.671306</td>\n      <td>11190</td>\n      <td>16669</td>\n      <td>12.681768</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.944400</td>\n      <td>1.161568</td>\n      <td>0.022702</td>\n      <td>[0.196672261548566, 0.03937276291119823, 0.015348648099207156, 0.005311920938851143]</td>\n      <td>0.805374</td>\n      <td>0.822065</td>\n      <td>13703</td>\n      <td>16669</td>\n      <td>15.633141</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.271200</td>\n      <td>1.139614</td>\n      <td>0.023969</td>\n      <td>[0.20201430448109764, 0.042071197411003236, 0.01632528898803488, 0.005655969506947006]</td>\n      <td>0.805302</td>\n      <td>0.822005</td>\n      <td>13702</td>\n      <td>16669</td>\n      <td>15.948955</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.067900</td>\n      <td>1.118648</td>\n      <td>0.025677</td>\n      <td>[0.2055050960271788, 0.04298660678482637, 0.016241737488196413, 0.005648441030275644]</td>\n      <td>0.855796</td>\n      <td>0.865259</td>\n      <td>14423</td>\n      <td>16669</td>\n      <td>15.489040</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.234700</td>\n      <td>1.112166</td>\n      <td>0.024984</td>\n      <td>[0.2130041518386714, 0.04444636977993415, 0.017314232711532516, 0.006105158235733865]</td>\n      <td>0.789908</td>\n      <td>0.809167</td>\n      <td>13488</td>\n      <td>16669</td>\n      <td>15.680401</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.969800</td>\n      <td>1.102233</td>\n      <td>0.025670</td>\n      <td>[0.18991006529505974, 0.038207613428852365, 0.01442660363517809, 0.004617604617604618]</td>\n      <td>0.973560</td>\n      <td>0.973904</td>\n      <td>16234</td>\n      <td>16669</td>\n      <td>18.810749</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.075900</td>\n      <td>1.086269</td>\n      <td>0.026364</td>\n      <td>[0.20686092167803266, 0.045200892857142856, 0.016842303349642453, 0.005550521069324876]</td>\n      <td>0.862224</td>\n      <td>0.870898</td>\n      <td>14517</td>\n      <td>16669</td>\n      <td>17.229205</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.072200</td>\n      <td>1.072942</td>\n      <td>0.026595</td>\n      <td>[0.19681834998150202, 0.040823496346261944, 0.01481843347988927, 0.004696185547249377]</td>\n      <td>0.972574</td>\n      <td>0.972944</td>\n      <td>16218</td>\n      <td>16669</td>\n      <td>18.443563</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.168900</td>\n      <td>1.061040</td>\n      <td>0.026375</td>\n      <td>[0.1901267903263677, 0.0386839481555334, 0.013911182450508293, 0.004730031236055332]</td>\n      <td>1.000000</td>\n      <td>1.022017</td>\n      <td>17036</td>\n      <td>16669</td>\n      <td>18.522093</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.120000</td>\n      <td>1.053778</td>\n      <td>0.031750</td>\n      <td>[0.19557725445146468, 0.04431036719865058, 0.017624749014650106, 0.006652842578192501]</td>\n      <td>1.000000</td>\n      <td>1.044454</td>\n      <td>17410</td>\n      <td>16669</td>\n      <td>19.757075</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.854600</td>\n      <td>1.043064</td>\n      <td>0.030420</td>\n      <td>[0.2085095794973874, 0.045735388111639796, 0.01774952530339305, 0.005863383172090296]</td>\n      <td>0.963785</td>\n      <td>0.964425</td>\n      <td>16076</td>\n      <td>16669</td>\n      <td>19.192813</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.133600</td>\n      <td>1.034731</td>\n      <td>0.030361</td>\n      <td>[0.20001138368717628, 0.043732340097611094, 0.016702229416525643, 0.005816439996578565]</td>\n      <td>1.000000</td>\n      <td>1.053992</td>\n      <td>17569</td>\n      <td>16669</td>\n      <td>19.790404</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.926100</td>\n      <td>1.025766</td>\n      <td>0.035405</td>\n      <td>[0.21200345423143352, 0.05008088907645776, 0.02048645119164218, 0.008086253369272238]</td>\n      <td>0.972205</td>\n      <td>0.972584</td>\n      <td>16212</td>\n      <td>16669</td>\n      <td>19.632532</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.149500</td>\n      <td>1.024901</td>\n      <td>0.033908</td>\n      <td>[0.22577696526508226, 0.052082551594746715, 0.02032198469253101, 0.007876496534341524]</td>\n      <td>0.915450</td>\n      <td>0.918831</td>\n      <td>15316</td>\n      <td>16669</td>\n      <td>19.808498</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.102300</td>\n      <td>1.016426</td>\n      <td>0.034259</td>\n      <td>[0.20798043659787666, 0.048121827411167516, 0.019280305987042386, 0.007138934651290499]</td>\n      <td>1.000000</td>\n      <td>1.005819</td>\n      <td>16766</td>\n      <td>16669</td>\n      <td>20.359816</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.087900</td>\n      <td>1.007875</td>\n      <td>0.034136</td>\n      <td>[0.2160260201064459, 0.05135425046929472, 0.019315460094259444, 0.006336561962523762]</td>\n      <td>1.000000</td>\n      <td>1.014458</td>\n      <td>16910</td>\n      <td>16669</td>\n      <td>21.040576</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.887000</td>\n      <td>0.996157</td>\n      <td>0.034297</td>\n      <td>[0.22404580152671755, 0.053609148517736176, 0.020732432662078342, 0.007074279939363315]</td>\n      <td>0.941417</td>\n      <td>0.943068</td>\n      <td>15720</td>\n      <td>16669</td>\n      <td>20.318746</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.981000</td>\n      <td>0.994868</td>\n      <td>0.038006</td>\n      <td>[0.22301548886737657, 0.05449291316912068, 0.021559268098647572, 0.008239700374531835]</td>\n      <td>0.991505</td>\n      <td>0.991541</td>\n      <td>16528</td>\n      <td>16669</td>\n      <td>20.993375</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.978800</td>\n      <td>0.986320</td>\n      <td>0.035880</td>\n      <td>[0.2172908560311284, 0.050096872405203434, 0.02026918763018747, 0.007926023778071334]</td>\n      <td>0.986654</td>\n      <td>0.986742</td>\n      <td>16448</td>\n      <td>16669</td>\n      <td>20.268620</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.908700</td>\n      <td>0.985702</td>\n      <td>0.036798</td>\n      <td>[0.2252099786887301, 0.05481120584652863, 0.021934945788156798, 0.008101165777514326]</td>\n      <td>0.956173</td>\n      <td>0.957106</td>\n      <td>15954</td>\n      <td>16669</td>\n      <td>20.763208</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3241\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m-> 3241\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:159\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"markdown","source":"# Uploading to Huggingface","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"HF_TOKEN\"] = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:34:34.126614Z","iopub.execute_input":"2024-06-29T12:34:34.126961Z","iopub.status.idle":"2024-06-29T12:34:34.132882Z","shell.execute_reply.started":"2024-06-29T12:34:34.126937Z","shell.execute_reply":"2024-06-29T12:34:34.131771Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli repo create opus_mt_en-bn-ft --y","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:08:05.304512Z","iopub.execute_input":"2024-06-29T11:08:05.305242Z","iopub.status.idle":"2024-06-29T11:08:07.517666Z","shell.execute_reply.started":"2024-06-29T11:08:05.305200Z","shell.execute_reply":"2024-06-29T11:08:07.516430Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[90mgit version 2.25.1\u001b[0m\n\u001b[90mgit-lfs/2.9.2 (GitHub; linux amd64; go 1.13.5)\u001b[0m\n\nYou are about to create \u001b[1mSarwarShafee/opus_mt_en-bn-ft\u001b[0m\n\nYour repo now lives at:\n  \u001b[1mhttps://huggingface.co/SarwarShafee/opus_mt_en-bn-ft\u001b[0m\n\nYou can clone it locally with the command below, and commit/push as usual.\n\n  git clone https://huggingface.co/SarwarShafee/opus_mt_en-bn-ft\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://SarwarShafee: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@huggingface.co/SarwarShafee/opus_mt_en-bn-ft","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:14:40.045993Z","iopub.execute_input":"2024-06-29T11:14:40.046450Z","iopub.status.idle":"2024-06-29T11:14:41.569137Z","shell.execute_reply.started":"2024-06-29T11:14:40.046414Z","shell.execute_reply":"2024-06-29T11:14:41.567805Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Cloning into 'opus_mt_en-bn-ft'...\nremote: Enumerating objects: 3, done.\u001b[K\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\nUnpacking objects: 100% (3/3), 1.07 KiB | 1.07 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd opus_mt_en-bn-ft\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:14:47.384494Z","iopub.execute_input":"2024-06-29T11:14:47.384916Z","iopub.status.idle":"2024-06-29T11:14:48.403346Z","shell.execute_reply.started":"2024-06-29T11:14:47.384883Z","shell.execute_reply":"2024-06-29T11:14:48.402197Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"/kaggle/working/opus_mt_en-bn-ft\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp /kaggle/working/checkpoint-200/* /kaggle/working/opus_mt_en-bn-ft\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:14:53.337170Z","iopub.execute_input":"2024-06-29T11:14:53.337605Z","iopub.status.idle":"2024-06-29T11:14:56.228529Z","shell.execute_reply.started":"2024-06-29T11:14:53.337570Z","shell.execute_reply":"2024-06-29T11:14:56.227022Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"config.json\t\tmodel.safetensors  rng_state.pth  trainer_state.json\ngeneration_config.json\toptimizer.pt\t   scheduler.pt   training_args.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"!git add .\n!git commit -m \"file added\"\n!git push","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:15:05.519988Z","iopub.execute_input":"2024-06-29T11:15:05.520439Z","iopub.status.idle":"2024-06-29T11:15:21.971602Z","shell.execute_reply.started":"2024-06-29T11:15:05.520402Z","shell.execute_reply":"2024-06-29T11:15:21.970377Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"[main 3c625ca] file added\n 8 files changed, 1985 insertions(+)\n create mode 100644 config.json\n create mode 100644 generation_config.json\n create mode 100644 model.safetensors\n create mode 100644 optimizer.pt\n create mode 100644 rng_state.pth\n create mode 100644 scheduler.pt\n create mode 100644 trainer_state.json\n create mode 100644 training_args.bin\nUploading LFS objects: 100% (5/5), 924 MB | 244 MB/s, done.                     \nEnumerating objects: 11, done.\nCounting objects: 100% (11/11), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (10/10), done.\nWriting objects: 100% (10/10), 10.01 KiB | 5.01 MiB/s, done.\nTotal 10 (delta 0), reused 0 (delta 0)\nTo https://huggingface.co/SarwarShafee/opus_mt_en-bn-ft\n   20eb63c..3c625ca  main -> main\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd ..\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:18:09.072106Z","iopub.execute_input":"2024-06-29T11:18:09.072515Z","iopub.status.idle":"2024-06-29T11:18:10.127328Z","shell.execute_reply.started":"2024-06-29T11:18:09.072484Z","shell.execute_reply":"2024-06-29T11:18:10.126226Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"/kaggle/working\ncheckpoint-160\tcheckpoint-180\tcheckpoint-200\t  wandb\ncheckpoint-170\tcheckpoint-190\topus_mt_en-bn-ft\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Load","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntranslate = pipeline('translation', model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:22:58.438074Z","iopub.execute_input":"2024-06-29T11:22:58.438733Z","iopub.status.idle":"2024-06-29T11:22:58.445297Z","shell.execute_reply.started":"2024-06-29T11:22:58.438694Z","shell.execute_reply":"2024-06-29T11:22:58.444136Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"translate(\"I will die soon\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:21:11.946077Z","iopub.execute_input":"2024-06-29T11:21:11.946571Z","iopub.status.idle":"2024-06-29T11:21:12.205073Z","shell.execute_reply.started":"2024-06-29T11:21:11.946533Z","shell.execute_reply":"2024-06-29T11:21:12.203866Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"[{'translation_text': 'আমি দিতে মতে মুরুণ'}]"},"metadata":{}}]},{"cell_type":"code","source":"## download for safty","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r checkpoint-200.zip checkpoint-200 ","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:49:46.781779Z","iopub.execute_input":"2024-06-29T11:49:46.782763Z","iopub.status.idle":"2024-06-29T11:50:37.858693Z","shell.execute_reply.started":"2024-06-29T11:49:46.782706Z","shell.execute_reply":"2024-06-29T11:50:37.857314Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"  adding: checkpoint-200/ (stored 0%)\n  adding: checkpoint-200/rng_state.pth (deflated 25%)\n  adding: checkpoint-200/scheduler.pt (deflated 56%)\n  adding: checkpoint-200/generation_config.json (deflated 43%)\n  adding: checkpoint-200/trainer_state.json (deflated 82%)\n  adding: checkpoint-200/training_args.bin (deflated 52%)\n  adding: checkpoint-200/config.json (deflated 61%)\n  adding: checkpoint-200/optimizer.pt (deflated 8%)\n  adding: checkpoint-200/model.safetensors (deflated 8%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!du -sh checkpoint-200.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:50:37.861346Z","iopub.execute_input":"2024-06-29T11:50:37.861802Z","iopub.status.idle":"2024-06-29T11:50:38.919216Z","shell.execute_reply.started":"2024-06-29T11:50:37.861760Z","shell.execute_reply":"2024-06-29T11:50:38.917819Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"814M\tcheckpoint-200.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:56:48.510166Z","iopub.execute_input":"2024-06-29T11:56:48.510634Z","iopub.status.idle":"2024-06-29T11:56:48.518839Z","shell.execute_reply.started":"2024-06-29T11:56:48.510596Z","shell.execute_reply":"2024-06-29T11:56:48.517695Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-mul', vocab_size=64110, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t64109: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}